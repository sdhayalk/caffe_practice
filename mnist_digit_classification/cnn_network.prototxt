# defining the name of this entire network
name: "cnn_network"

# defining the data layer, which reads MNIST training data from the lmdb
# 	it takes input as type Data
# 	it produces two blobs: data blob and label blob
# 	the scale is a number that will be multiplied with all the values (pixel values), thus making then range from [0,1]. It is equivalent to 1/256
layer {
  name: "mnist"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "G:/DL/mnist_data_for_caffe/dataset_train.txt"
    batch_size: 64
  }
}

# defining the data layer, which reads MNIST testing data from the lmdb
# 	it takes input as type Data
# 	it produces two blobs: data blob and label blob
layer {
  name: "mnist"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "G:/DL/mnist_data_for_caffe/dataset_train.txt"
    batch_size: 64
  }
}

# defining the first conv layer, with name conv1
#	it is of type Convolution, taking bottom data and giving top conv1(itself)
# 	convolution params are 3x3 filter with 32 output channels, stride 1, using xavier initialization for weights and constant initialization for bias i.e. 0 (important)
# 	there are two param { }. lr_mults are the learning rate adjustments for the layerâ€™s learnable parameters. In this case, we will set the weight learning rate to be the same as the learning rate given by the solver during runtime, and the bias learning rate to be twice as large as that - this usually leads to better convergence rates.
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

# defining first activation layer, with name relu1
#	it has type ReLU, with top and bottom as same(Since ReLU is an element-wise operation, we can do in-place operations to save some memory)
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}

# defining first pooling layer, with name pool1
# 	the pooling params are: max pooling with 2x2 kernel with stride 2
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

# defining second convolution layer, with name conv2
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

# defining second activation layer, with name relu2
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}

# defining second pooling layer, with name pool2
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

# defining third convolution layer, with name conv3
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

# defining third activation layer, with name relu3
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}

# defining third pooling layer, with name pool3
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

# defining first fully connected layer, also called as inner product, with name ip1
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

# defining second fully connected layer, also called as inner product, with name ip2
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

#layer {
#  name: "accuracy"
#  type: "Accuracy"
#  bottom: "ip2"
#  bottom: "label"
#  top: "accuracy"
#  include {
#    phase: TEST
#  }
#}
layer {
  name: "loss"
  type: "SigmoidCrossEntropyLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}